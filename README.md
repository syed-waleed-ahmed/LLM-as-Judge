# ğŸ¤– LLM-as-Judge Evaluation Framework (Groq)

A fully interactive Streamlit app that uses a **Groq-hosted LLM (Llama 3 models)** to act as an **impartial judge** for evaluating and comparing two model outputs.  
This tool helps you measure subjective qualitiesâ€”such as *creativity*, *brand tone adherence*, *code readability*, or any custom metric you define.

Perfect for:
- A/B testing model responses  
- Benchmarking prompt variations  
- Building custom evaluation frameworks  
- Automating subjective model scoring  

---

## ğŸš€ Features

### ğŸ” Compare Model Outputs
Paste two outputs (A and B), describe the task, and let the LLM judge evaluate them.

### ğŸ¯ Multiple Evaluation Criteria
Choose from built-in presets:
- Creativity
- Brand Tone Adherence
- Code Readability

Or define **your own custom criterion** (e.g., "Factual Accuracy", "Politeness", "Conciseness", etc.).

### ğŸ“Š Structured Evaluation Output
Every evaluation returns:
- Score for Output A
- Score for Output B
- Judge's explanation for each
- A final winner (A, B, or tie)
- A combined overall comment

### ğŸ§  Powered by Groq
Uses the Groq API for ultra-fast inference with **Llama 3** models.

### ğŸ–¥ï¸ Clean UI
Built entirely with Streamlit â€” responsive and easy to use.

---

## ğŸ—‚ï¸ Project Structure
```text
llm-as-judge-groq/
â”œâ”€ app.py # Streamlit web UI
â”œâ”€ judge.py # Groq evaluation logic
â”œâ”€ prompts.py # Prompt templates and criteria presets
â”œâ”€ config.py # Loads environment variables
â”œâ”€ requirements.txt # Python dependencies
â”œâ”€ .env.example # Example env file (no real keys)
â”œâ”€ .gitignore # Ignore secrets and build files
â””â”€ README.md # Project documentation
```

## ğŸ§ª Usage Example

1. Describe the task:
"Write a friendly tweet announcing our new AI tool."
2. Paste two different outputs from any models.
3. Select a criterion (e.g., Creativity)
4. Click Evaluate Outputs

Youâ€™ll receive:

- A score for Output A
- A score for Output B
- Explanations
- A final winner
- Overall comments

## ğŸ”’ Security Notes

- Never commit your .env file.
- Your .gitignore already ignores .env.
- If a key is ever exposed, regenerate it immediately in the Groq console.

## ğŸ› ï¸ Future Improvements (ideas)

- ğŸ“ Multi-criteria scoring (e.g., Creativity + Accuracy + Tone)
- ğŸ“¥ Upload CSV for batch evaluation
- ğŸ“¤ Export results (CSV/JSON)
- ğŸ§ª Integrate with experiment tracking (Weights & Biases, LangSmith, etc.)
- ğŸ”Œ Plug in more models (OpenAI, Anthropic, HuggingFace, etc.)

## â­ Contributing

- Pull requests are welcome!
- Feel free to submit issues or feature requests.

## ğŸ“„ License

- MIT License â€” free to use, modify, and distribute.

Enjoy using the LLM-as-Judge Evaluation Framework! If you build something cool on top of this, share it! ğŸš€

## Author

Created by Syed Waleed Ahmed